{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757fe57a",
   "metadata": {},
   "source": [
    "# Sesi√≥n 14 A\n",
    "\n",
    "## Modelos de Mezcla Gaussiana (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54c25f",
   "metadata": {},
   "source": [
    "> **Objetivos:**\n",
    ">\n",
    "> - Introducir K-Means como contraste simple a GMM.\n",
    "> - Familiarizarse con el algoritmo Expectation-Maximization (EM) para GMM.\n",
    "> - Explorar GMMs.\n",
    "\n",
    "> **Lectura recomendada:**\n",
    ">\n",
    "> Mixture Models and EM. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc24d6",
   "metadata": {},
   "source": [
    "### 1. K-Means Clustering\n",
    "\n",
    "#### 1.1. El modelo\n",
    "\n",
    "Dado un conjunto de datos:\n",
    "\n",
    "$$\n",
    "X = \\{x_1, x_2, \\ldots, x_N\\}, \\qquad x_n \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "Queremos particionarlos en $K$ clusters definidos por sus **centroides**:\n",
    "\n",
    "$$\n",
    "\\mu_1, \\mu_2, \\ldots, \\mu_K \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "Introducimos variables de asignaci√≥n:\n",
    "$$\n",
    "r_{nk} \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "donde  \n",
    "- $r_{nk} = 1$ si el punto $x_n$ est√° asignado al cluster $k$  \n",
    "- cada punto pertenece a un √∫nico cluster:\n",
    "$$\n",
    "\\sum_{k=1}^K r_{nk} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bb71d",
   "metadata": {},
   "source": [
    "#### 1.2. Funci√≥n de costo (distorsi√≥n / inertia / WCSS)\n",
    "\n",
    "La funci√≥n objetivo que queremos minimizar es:\n",
    "\n",
    "$$\n",
    "J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\, \\|x_n - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "Esta mide la **suma de distancias cuadradas dentro del cluster**.  \n",
    "\n",
    "Minimizar $J$ ‚Üí significa clusters compactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b375fb",
   "metadata": {},
   "source": [
    "> K-means es geom√©trico, no probabil√≠stico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd6be2",
   "metadata": {},
   "source": [
    "#### 1.3. Algoritmo de optimizaci√≥n: algoritmo de Lloyd\n",
    "\n",
    "K-means minimiza $J$ mediante un proceso iterativo de **descenso alternado**:\n",
    "\n",
    "* **(A) Paso de asignaci√≥n**\n",
    "Fijando los centroides, asignamos cada punto al m√°s cercano:\n",
    "\n",
    "$$\n",
    "r_{nk} =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } k = \\arg\\min_j \\|x_n - \\mu_j\\|^2 \\\\\n",
    "0 & \\text{otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* **(B) Paso de actualizaci√≥n de centroides**\n",
    "\n",
    "Fijando las asignaciones, cada centroide se actualiza como el **promedio** de sus puntos:\n",
    "\n",
    "$$\n",
    "\\mu_k =\n",
    "\\frac{\n",
    "\\sum_{n=1}^N r_{nk} \\, x_n\n",
    "}{\n",
    "\\sum_{n=1}^N r_{nk}\n",
    "}\n",
    "$$\n",
    "\n",
    "* **(C) Criterio de convergencia**\n",
    "\n",
    "El algoritmo termina cuando:\n",
    "- las asignaciones no cambian, o  \n",
    "- los centroides dejan de moverse, o  \n",
    "- se alcanza un m√°ximo de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55040d6",
   "metadata": {},
   "source": [
    "üî• <span style=\"color:#4f4559;\">**Ejercicio en pizarron**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362920f",
   "metadata": {},
   "source": [
    "![](../images/sesion14-img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78283df",
   "metadata": {},
   "source": [
    "**Figura 1:** (a) Los puntos verdes representan el conjunto de datos en un espacio euclidiano bidimensional. Las elecciones iniciales para los centros $\\mu_1$ y $\\mu_2$ se muestran con las cruces roja y azul, respectivamente.(b) En el paso **E** inicial, cada punto de datos se asigna al cl√∫ster rojo o al cl√∫ster azul, seg√∫n cu√°l centroide est√© m√°s cerca. Esto es equivalente a clasificar los puntos seg√∫n de qu√© lado de la **bisectriz perpendicular** entre los dos centros de cl√∫ster ‚Äîmostrada por la l√≠nea magenta‚Äî se encuentren. (c) En el paso **M** posterior, cada centro de cl√∫ster se recalcula como la media de los puntos asignados al cl√∫ster correspondiente.(d)‚Äì(i) muestran los pasos E y M sucesivos hasta la convergencia final del algoritmo. Retomada de Bishop (2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74dd30",
   "metadata": {},
   "source": [
    "#### 1.4. ¬øPor qu√© se le llama **\"hard-clustering\"**?\n",
    "\n",
    "Porque cada punto se asigna a un √∫nico cluster (asignaci√≥n dura).\n",
    "\n",
    "Anteriormente vimos en el paso de asignaci√≥n:\n",
    "$$\n",
    "r_{nk} =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } k = \\arg\\min_j \\|x_n - \\mu_j\\|^2 \\\\\n",
    "0 & \\text{otro caso}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689a32e",
   "metadata": {},
   "source": [
    "#### 1.5. ¬øQu√© asume el modelo K-means sobre los datos?\n",
    "\n",
    "![](../images/sesion14-img1.png)\n",
    "\n",
    "**Figura 2:** K-means asume que los clusters son esf√©ricos y de tama√±o similar, como ilustra la imagen de la izquierda. En la imagen de la derecha, K-means no puede capturar la estructura real de los datos debido a estas suposiciones. Retomada de Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98461d",
   "metadata": {},
   "source": [
    "- Los clusters son esf√©ricos (isotr√≥picos) y de tama√±o similar.\n",
    "- Fronteras de decisi√≥n lineales (basadas en distancia euclidiana).\n",
    "- Forma uniforme (sim√©trica alrededor del centroide).\n",
    "- M√°s puntos al centro del cluster que en los bordes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64774514",
   "metadata": {},
   "source": [
    "> [Aqu√≠](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) puedes ver la documentaci√≥n oficial de m√©todos de clustering en `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13a76",
   "metadata": {},
   "source": [
    "```{admonition} Nota\n",
    ":class: tip\n",
    "\n",
    "K-means funciona bien si los clusters son:\n",
    "- isotr√≥picos (no el√≠pticos)\n",
    "- convexos\n",
    "- de tama√±o parecido\n",
    "- con densidad decreciente radialmente\n",
    "\n",
    "Falla si los clusters son:\n",
    "- el√≠pticos\n",
    "- rotados\n",
    "- de distinta varianza\n",
    "- no convexos\n",
    "- solapados\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8d6c4",
   "metadata": {},
   "source": [
    "![](../images/sesion14-img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ed188",
   "metadata": {},
   "source": [
    "**Figura 3:** Ejemplos de conjuntos de datos donde K-means puede fallar debido a sus suposiciones sobre la forma y distribuci√≥n de los clusters. Retomada de Mael Fabien, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fbbb6",
   "metadata": {},
   "source": [
    "> * üí° **¬øNos hemos preguntado por qu√© sucede lo de cl√∫sters esf√©ricos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1101fb",
   "metadata": {},
   "source": [
    "### 2. Gaussian Mixture Models (GMMs) y K-means\n",
    "\n",
    "K-means es un m√©todo simple, r√°pido y muy √∫til cuando los cl√∫sters son ‚Äúredondos‚Äù, de tama√±o similar y bien separados. Pero estas suposiciones son fuertes y en muchos problemas reales **no se cumplen**.\n",
    "\n",
    "Esto nos deja con dos limitaciones importantes:\n",
    "\n",
    "1. **La forma del cluster est√° restringida.**  \n",
    "\n",
    "2. **La asignaci√≥n de los puntos es dura.**  \n",
    "\n",
    "Ahora, planteemos una pregunta natural:\n",
    "\n",
    "### _¬øY si pudi√©ramos relajar estas dos restricciones?_\n",
    "\n",
    "<details>\n",
    "<summary> Soft-clustering </summary>\n",
    "\n",
    "- Para la forma:  \n",
    "  **¬øQu√© define realmente la forma de un cluster?**  \n",
    "\n",
    "- Para la asignaci√≥n:  \n",
    "  **¬øTiene sentido obligar a que cada punto pertenezca a un solo cluster?**\n",
    "\n",
    "</details>\n",
    "\n",
    "Estas dos ideas ‚Äîmodelar la forma y permitir asignaciones suaves‚Äî nos llevan directamente a los **Gaussian Mixture Models (GMMs)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9704b9",
   "metadata": {},
   "source": [
    "Aqu√≠ van algunas pistas, si comparamos K-means y GMMs, tenemos que:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527614d",
   "metadata": {},
   "source": [
    "| k-Means | GMM (Gaussian Mixture Model) |\n",
    "|--------|-------------------------------|\n",
    "| Los cl√∫sters se definen por sus **medias** | Los cl√∫sters se definen por sus **medias y sus varianzas**, modelados como Gaussianas |\n",
    "| Tiene limitaciones si los cl√∫sters est√°n **superpuestos** | Funciona incluso si los cl√∫sters est√°n **superpuestos** |\n",
    "| Utiliza la **distancia euclidiana** al centroide | Utiliza la **probabilidad** de que X pertenezca a un cl√∫ster (modelo generativo) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d6078",
   "metadata": {},
   "source": [
    "El algoritmo **k-Means** es, de hecho, un **caso especial** de un GMM con *Expectation-Maximization: Hard*, donde cada componente se modela mediante  \n",
    "\n",
    "$$\n",
    "\\mathcal N(\\mu_k, I),\n",
    "$$\n",
    "\n",
    "siendo $I$ la matriz identidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b68ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21df033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generemos dos cl√∫sters\n",
    "\n",
    "np.random.seed(0)\n",
    "n1 = 500\n",
    "n2 = 500\n",
    "\n",
    "#Cl√∫ster 1\n",
    "mu1 = [0, 0]\n",
    "cov1 =[[1,0],\n",
    "       [0,1]]\n",
    "\n",
    "C1 = np.random.multivariate_normal(mu1, cov1, n1)\n",
    "\n",
    "#Cl√∫ster 2\n",
    "mu2 = [5, 0]\n",
    "cov2 =[[16,5],\n",
    "       [4,12]]\n",
    "\n",
    "C2 = np.random.multivariate_normal(mu2, cov2, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1539484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar np.vstack\n",
    "X = np.vstack([C1, C2])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafiquemos los datos\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.scatter(C1[:, 0], C1[:, 1], color='blue', alpha=0.5, label='Cluster 1 (Esf√©rico)')\n",
    "plt.scatter(C2[:, 0], C2[:, 1], color='red',  alpha=0.5, label='Cluster 2 (Elongado)')\n",
    "\n",
    "plt.title(\"Clusters con covarianza esf√©rica vs. covarianza no esf√©rica\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17603c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-means\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X)\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar resultado de KMeans\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroides')\n",
    "\n",
    "plt.title(\"Resultado de KMeans sobre los clusters\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e96db",
   "metadata": {},
   "source": [
    "### 3. GMMs\n",
    "\n",
    "Los GMMs son **modelos probabil√≠sticos** que asumen que los datos son generados a partir de una **mezcla** de varias distribuciones Gaussianas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd861e9a",
   "metadata": {},
   "source": [
    "#### 3.1. ¬øPor qu√© una _\"mezcla\"_ de Gaussianas?\n",
    "\n",
    "Pensemos en el siguiente ejemplo:\n",
    "\n",
    "![](../images/sesion14-img4.png)\n",
    "\n",
    "**Figura 4:** Ejemplo de un conjunto de datos que parece obvio que proviene de dos grupos distintos. Si modelamos con una sola distribucui√≥n Gaussiana, quiz√° terminemos con un _promedio_ que no refleje la estructura real de los datos. Retomada de Mael Fabien, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646f669",
   "metadata": {},
   "source": [
    "#### 3.1. Set up\n",
    "\n",
    "Tenemos datos $X = \\{x_1, x_2, \\ldots, x_N\\}$ que parecen venir de varios grupos, pero:\n",
    "\n",
    "* No sabemos cu√°ntos grupos generaron los datos.\n",
    "* No sabemos a qu√© grupo pertenece cada punto.\n",
    "* No sabemos ni las medias ni las formas (var/cov) de esos grupos.\n",
    "\n",
    "En un GMM, suponemos que:\n",
    "\n",
    "> Cada grupo o componente es una distribuci√≥n Gaussiana con sus propios par√°metros (media y covarianza)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bc4db",
   "metadata": {},
   "source": [
    "Para aprender el modelo, necesitamos 3 par√°metros por componente:\n",
    "\n",
    "* su media $\\mu_k$\n",
    "* su varianza/covarianza $\\sigma_k$ o $\\Sigma_k$\n",
    "* su peso $w_k=p(z=k)$ (probabilidad de elegir ese grupo)\n",
    "\n",
    "$$\\theta = \\{ \\mu_k, \\Sigma_k, w_k \\}_{k=1}^K$$\n",
    "\n",
    "> estos par√°metros se **inicializan aleatoriamente** o con alguna otra t√©cnica y se aprenden a partir de los datos usando el algoritmo de Expectation-Maximization (EM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a206269",
   "metadata": {},
   "source": [
    "![](../images/sesion14-img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7e7a8",
   "metadata": {},
   "source": [
    "**Figura 5:** Se muestran tres distribuciones gaussianas (componentes de un modelo). Notese que cada gaussiana tiene un valor para sus par√°metros $(\\mu_k, \\Sigma_k)$ y un peso $w_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706be46",
   "metadata": {},
   "source": [
    "#### 3.2. ¬øC√≥mo nos ayuda la probabilidad y los grafos probabil√≠sticos?\n",
    "\n",
    "El objetivo principal de un Modelo de Mezcla Gaussiana (GMM) es poder calcular la _probabilidad de observar un dato $x$_ bajo el modelo:\n",
    "\n",
    "$$\n",
    "p(x)\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "> es decir, queremos saber qu√© tan probable es que el dato $x$ haya sido generado por nuestro modelo.\n",
    "\n",
    "Pero hay un problema: \n",
    "\n",
    "* El dato $x$ s√≠ lo observamos, \n",
    "* El componente del que proviene, $z$, **no lo observamos**.\n",
    "\n",
    "Por eso introducimos un concepto importante: **la variable latente** $z_n$.\n",
    "\n",
    "![](../images/sesion14-img5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a17af",
   "metadata": {},
   "source": [
    "**Figura 6:** Diagrama de un modelo gr√°fico probabil√≠stico para un GMM con $K$ componentes. Retomade de Bishop (2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8b2c5",
   "metadata": {},
   "source": [
    "**¬øQu√© representa la variable latente $z_n$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11859a59",
   "metadata": {},
   "source": [
    "La variable $z_n$ es \"latente\" porque **no se observa** directamente. Su funci√≥n es indicar a cu√°l componente del GMM pertenece el dato $x_n$.\n",
    "\n",
    "En un modelo de mezclas gaussianas, esta relaci√≥n suele representarse mediante un **grafo probabil√≠stico** como el mostrado en la Figura 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b30fd",
   "metadata": {},
   "source": [
    "En este grafo se expresa la distribuci√≥n conjunta como:\n",
    "\n",
    "$$\n",
    "p(x,z)=p(z)p(x|z)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "El nodo $z$ genera (o explica) al nodo $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9744f0f",
   "metadata": {},
   "source": [
    "**¬øc√≥mo obtenemos $p(x) si $z$ no es observable?**\n",
    "\n",
    "La _probabilidad total_ nos da  la clave: si hay variables ocultas, para obtener $p(x)$ debemos _sumar_ sobre todos los valores posibles de la variable oculta:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_z p(x, z)\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    ">Si no s√© qu√© gener√≥ el dato $x$, considero todos los posibles or√≠genes.\n",
    "\n",
    "Luego, si factorizamos la distribuci√≥n conjunta usando la _regla de la cadena_:\n",
    "\n",
    "$$\n",
    "p(x,z) = p(z) \\, p(x|z)\\tag{4}\n",
    "$$\n",
    "\n",
    "y sustituimos, obtenemos:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_z p(z) \\, p(x|z)\\tag{5}\n",
    "$$\n",
    "\n",
    ">Esta es la idea esencial! Aunque no separamos qu√© componente gener√≥ el dato, podemos considerar todos los componentes y ponderarlos por su probabilidad.\n",
    "\n",
    "**Modelo GMM:**\n",
    "\n",
    "En un GMM, los valores posibles de $z$ corresponde a los $K$ componentes de la mezcla.\n",
    "\n",
    "Cada componente tiene: \n",
    "\n",
    "* un peso $w_k = p(z=k)$\n",
    "* una distribuci√≥n Gaussiana $p(x|z=k) = \\mathcal{N}(x | \\mu_k, \\Sigma_k)$\n",
    "\n",
    "Sustituyendo en la f√≥rmula, obtenmos el modelo GMM:\n",
    "\n",
    "$$ \\boxed{p(x) = \\sum_{k=1}^K w_k \\, \\mathcal{N}(x | \\mu_k, \\Sigma_k)}\\tag{6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe75a4",
   "metadata": {},
   "source": [
    "El modelo en $(6)$ dice que la probabilidad de $x$ es una suma ponderada de varias Gaussianas: cada una aporta seg√∫n qu√© tan probable es que ese componente sea el responsable de generar $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d9bbd",
   "metadata": {},
   "source": [
    "#### 3.3. ¬øDe qu√© componente vino ese dato?\n",
    "\n",
    "Hasta ahora sabemos c√≥mo calcula un GMM la probabilidad total de un dato $x$. Pero el siguiente paso es m√°s interesante:\n",
    "\n",
    "* **Inferencia** de $Z$: queremos inferir de qu√© componente $k$ de la mezcla proviene un dato $x_i$.\n",
    "\n",
    "> ¬øCu√°l es la probabilidad de que el dato $x_i$ haya sido generado por el componente $k$?\n",
    "\n",
    "A esta probabilidad se le llama _resposability_ (responsabilidad) del componente $k$ sobre el dato $x_i$:\n",
    "\n",
    "$$ \n",
    "\\gamma_{zk} = p(z_k = k | x_i)\\tag{7}\n",
    "$$\n",
    "\n",
    "Aplicamos el teorema de Bayes:\n",
    "\n",
    "$$\n",
    "\\gamma_{zk} = \\frac{p(z_k=k) \\, p(x_i | z_k=k)}{p(x_i)}\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "> y esto tiene una lectura muy intuitiva:\n",
    ">\n",
    "> * $p(z=k)$: lo probable que es el componente $k$ antes de ver el dato (prior)\n",
    "> * $p(x_i|z=k)$: qu√© tan bien explica el componente $k$ el dato.\n",
    "> * $p(x_i)$: normaliza la probabilidad para sumar 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af2f3b",
   "metadata": {},
   "source": [
    "En la **Figura 6** imaginamos un dato $x_i$ y tres gaussianas distintas. Cada gaussiana tiene una **altura** diferente en el punto $x_i$.\n",
    "\n",
    "> La altura de cada gaussiana en $x_i$ describe qu√© tan probable es ese dato si ese componente lo hubiere generado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee771122",
   "metadata": {},
   "source": [
    "![](../images/sesion14-img7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77176b7",
   "metadata": {},
   "source": [
    "**Figura 6:** La figura representa un dato $x_i$. Hay 3 distribuciones gaussianas del modelo $k=1,2,3$. Cada gaussiana tiene un **valor distinto** en ese punto $x_i$. Cada dato est√° asociado (o no) a varias Gaussianas. **La altura relativa de cada Gaussiana en la posici√≥n del dato determina la probabilidad de que ese dato pertenezca a ese componente.** Esa probabilida es precisamente $\\gamma_{ik} = p(z_k = k | x_i)$. Retomada de Mael Fabien, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ede74",
   "metadata": {},
   "source": [
    "Sustityendo cada t√©rmino usando el modelo GMM:\n",
    "\n",
    "* Prior (peso del componente): $p(z_k=k) = w_k$\n",
    "* Likelihood (gaussiana del componente): $p(x_i | z_k=k) = \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$\n",
    "* Evidencia (prob. total del dato): $p(x_i) = \\sum_{c=1}^K w_c \\, \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d2002",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma_{ik} = \\frac{w_k \\, \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{c=1}^K w_c \\, \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)}\\tag{9}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edf93c",
   "metadata": {},
   "source": [
    "#### 3.4. Algoritmo Expectation-Maximization (EM)\n",
    "\n",
    "Esto nos lleva de forma natural al algoritmo EM, un m√©todo elegante y poderoso para encontrar soluciones de m√°xima verosimilitud en modelos con variables latentes.\n",
    "\n",
    "El algoritmo EM alterna dos pasos: **Estimaci√≥n** y **Maximizaci√≥n**.\n",
    "  \n",
    "La idea general puede visualizarse como en la **Figura 7**:\n",
    "\n",
    "![](../images/sesion14-img8.png)\n",
    "\n",
    "**Figura 7:** Diagrama del algoritmo Expectation-Maximization (EM) para Gaussian Mixture Models (GMMs). Retomada de Mael Fabien, 2020.\n",
    "\n",
    "- Partimos de unos **par√°metros iniciales** $\\theta^{(t)}$ del modelo.\n",
    "\n",
    "- En el **E-step (Estimation)**, mantenemos fijos esos par√°metros y calculamos las  $\\gamma_{ik}^{(t)}$, es decir, la probabilidad de que cada dato haya sido generado por cada componente.\n",
    "\n",
    "- En el **M-step (Maximization)**, mantenemos fijas las  $\\gamma_{ik}^{(t)}$ y actualizamos los par√°metros del modelo para obtener $\\theta^{(t+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ed9d3",
   "metadata": {},
   "source": [
    "* **E-step:**\n",
    "\n",
    "En el algoritmo EM, cada iteraci√≥n tiene par√°metros actuales \n",
    "\n",
    "$$\\theta^{(t)} = \\{\\, w_k^{(t)},\\, \\mu_k^{(t)},\\, \\Sigma_k^{(t)} \\,\\}$$\n",
    "\n",
    "El **E-step** consiste precisamente en **evaluar la expresi√≥n anterior usando los par√°metros actuales**, es decir:\n",
    "\n",
    "$$\n",
    "\\gamma_{ik}^{(t)}\n",
    "=\n",
    "p(z_i = k \\mid x_i, \\theta^{(t)})\n",
    "=\n",
    "\\frac{\n",
    "w_k^{(t)} \\, \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})\n",
    "}{\n",
    "\\sum_{c=1}^{K} \n",
    "w_c^{(t)} \\, \\mathcal{N}(x_i \\mid \\mu_c^{(t)}, \\Sigma_c^{(t)})\n",
    "}\n",
    "$$\n",
    "\n",
    "Estas probabilidades $\\gamma_{ik}^{(t)}$ son la **salida del E-step** y se usar√°n como ‚Äúasignaciones suaves‚Äù en el siguiente paso, el **M-step**, para actualizar los par√°metros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862583c7",
   "metadata": {},
   "source": [
    "> E-step calculamos $\\gamma_{ik} = p(z_k = k | x_i, \\theta^{(t)})$ usando los par√°metros actuales $\\theta^{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a984070",
   "metadata": {},
   "source": [
    "* **M-step:** \n",
    " \n",
    "Una vez calculadas las $\\gamma_{ik}^{(t)}$ en el E-step, el **M-step** mantiene fijas estas probabilidades y actualiza los par√°metros del modelo para obtener nuevos valores\n",
    "\n",
    "$$\\theta^{(t+1)} = \\{\\, w_k^{(t+1)},\\, \\mu_k^{(t+1)},\\, \\Sigma_k^{(t+1)} \\,\\}$$\n",
    "\n",
    "En esta etapa, cada dato contribuye a los par√°metros de cada componente de forma **ponderada** por su $\\gamma_{ik}^{(t)}$.\n",
    "\n",
    "El resultado es equivalente a realizar una **estimaci√≥n de m√°xima verosimilitud**, pero donde cada dato tiene un peso diferente seg√∫n qu√© componente lo explica mejor.\n",
    "\n",
    "Las actualizaciones toman la forma:\n",
    "\n",
    "- Nuevos pesos:\n",
    "  $$\n",
    "  w_k^{(t+1)} = \\frac{1}{N}\\sum_{i=1}^N \\gamma_{ik}^{(t)}\n",
    "  $$\n",
    "\n",
    "- Nuevas medias:\n",
    "  $$\n",
    "  \\mu_k^{(t+1)} =\n",
    "  \\frac{\\sum_{i=1}^N \\gamma_{ik}^{(t)} \\, x_i}{\\sum_{i=1}^N \\gamma_{ik}^{(t)}}\n",
    "  $$\n",
    "\n",
    "- Nuevas covarianzas:\n",
    "  $$\n",
    "  \\Sigma_k^{(t+1)} =\n",
    "  \\frac{\n",
    "    \\sum_{i=1}^N \\gamma_{ik}^{(t)}\n",
    "    (x_i - \\mu_k^{(t+1)})(x_i - \\mu_k^{(t+1)})^\\top\n",
    "  }{\n",
    "    \\sum_{i=1}^N \\gamma_{ik}^{(t)}\n",
    "  }\n",
    "  $$\n",
    "\n",
    "Con estos par√°metros actualizados $\\theta^{(t+1)}$, comienza una nueva iteraci√≥n del algoritmo: volvemos al E-step, calculamos nuevas $\\gamma_{ik}^{(t+1)}$, y repetimos el ciclo hasta que el modelo converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd384335",
   "metadata": {},
   "source": [
    "![](../images/sesion14-img9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefa52b",
   "metadata": {},
   "source": [
    "**Figura 8:** Visualizaci√≥n del proceso iterativo del algoritmo Expectation-Maximization (EM) para Gaussian Mixture Models (GMMs). En cada iteraci√≥n, el E-step calcula las responsabilidades $\\gamma_{ik}$ basadas en los par√°metros actuales del modelo, y el M-step actualiza los par√°metros utilizando estas responsabilidades. Este ciclo se repite hasta la convergencia del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228b793",
   "metadata": {},
   "source": [
    "> Notas para revisar a fondo el algoritmo EM:\n",
    "> - [Mixture Models and EM. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "> - [Columbia University](https://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf)\n",
    "> - [Wiki](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n",
    "> - [EM-GMM-HMM](https://github.com/maelfabien/EM_GMM_HMM?tab=readme-ov-file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
