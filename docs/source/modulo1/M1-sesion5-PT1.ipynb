{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e44f84",
   "metadata": {},
   "source": [
    "# Sesi√≥n 5 A\n",
    "\n",
    "## El error (Œµ) en regresi√≥n lineal: de la geometr√≠a a la estad√≠stica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556a9a5",
   "metadata": {},
   "source": [
    "> **Objetivos de la clase:**\n",
    "> - Recordar el ajuste de curvas polinomiales.\n",
    "> - Entender el fen√≥meno de overfitting en casos pr√°cticos.\n",
    "> - Explicar los m√≠nimos cuadrados ordinaros mediante el principio de m√°xima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcceaefb",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n\n",
    "\n",
    "Supongamos que tenemos un conjunto de entrenamiento con $N$ observaciones de $x$, \n",
    "\n",
    "$$[x_1, \\dots, x_N],$$\n",
    "\n",
    "en conjunto con las observaciones correspondientes de la variable objetivo $y$, \n",
    "\n",
    "$$[y_1, \\dots, y_N].$$\n",
    "\n",
    "En la siguiente gr√°fica mostramos datos de entrenamiento, con $N=20$. Estos datos se generaron eligiendo $x$ uniformemente espaciados en el intervalo $[0, 1]$, y la variable objetivo $y$ como el resultado de la funci√≥n $\\sin (2 \\pi x)$ m√°s un peque√±o ruido distribuido normal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9746ee",
   "metadata": {},
   "source": [
    "```{thebe-button}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siembra una semilla\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los datos (ficticios) del problema anterior\n",
    "N = 21\n",
    "\n",
    "x = np.linspace(0, 1, N)\n",
    "y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c042f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fica de los datos\n",
    "plt.plot(x, y, 'o', label='Datos', color='purple')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ec491",
   "metadata": {},
   "source": [
    "*Objetivo:* \n",
    "\n",
    "Explotar estos datos de entrenamiento para hacer predicciones $\\hat{y}$ de la variable objetivo para alg√∫n nuevo valor de la variable de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f999dcf",
   "metadata": {},
   "source": [
    "*Matem√°ticamente:*\n",
    "\n",
    "Antes de **formular el problema de forma probabil√≠stica**, procedamos de forma m√°s intuitiva. Lo que queremos hacer es ajustar a los datos una funci√≥n polinomial de la forma:\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M} = \\sum_{j=0}^{M} w_j x^j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63454e26",
   "metadata": {},
   "source": [
    "Notemos que aunque $f$ es una funci√≥n no lineal de $x$, es una funci√≥n **lineal respecto a los coeficientes $w$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3706191",
   "metadata": {},
   "source": [
    "```{admonition} Linealidad del modelo (respecto a los par√°metros)\n",
    ":class: tip\n",
    "\n",
    "**¬øQu√© significa que una funci√≥n sea lineal respecto a sus par√°metros?**\n",
    "\n",
    "Una funci√≥n es *lineal en los par√°metros* cuando los coeficientes aparecen *sin ser multiplicados entre s√≠, elevados a potencias ni dentro de funciones no lineales*.  \n",
    "\n",
    "En otras palabras, la expresi√≥n puede escribirse como:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 g_0(x) + \\beta_1 g_1(x) + \\dots + \\beta_k g_k(x),\n",
    "$$\n",
    "\n",
    "donde $g_i(x)$ son funciones conocidas de las variables $x$.  \n",
    "\n",
    "- Ejemplo lineal en los par√°metros:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 + \\beta_1 x + \\beta_2 x^2,\n",
    "$$\n",
    "\n",
    "(lineal en $\\beta_0, \\beta_1, \\beta_2$, aunque no en $x$).  \n",
    "\n",
    "- Ejemplo **no** lineal en los par√°metros:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 + e^{\\beta_1 x},\n",
    "$$\n",
    "\n",
    "porque $\\beta_1$ aparece dentro de una funci√≥n exponencial.  \n",
    "\n",
    "Esta distinci√≥n es crucial: si un modelo es lineal en los par√°metros, se pueden usar m√©todos como *(OLS)* para estimarlos de forma directa.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbf459",
   "metadata": {},
   "source": [
    "Los valores de los *coeficientes (par√°metros, pesos)* ser√°n determinados *ajustando el polinomio a los datos de entrenamiento*. Esto se puede lograr minimizando una **funci√≥n de error** (de costo o de p√©rdida) que mide la falta de ajuste entre la funci√≥n $f(x, w)$ y los datos de entrenamiento.\n",
    "\n",
    "Una elecci√≥n com√∫nmente usada para esta funci√≥n de error est√° dada por la suma de cuadrados de los errores entre las predicciones sobre los datos de entrenamiento $f(x_n,w)$ y los valores correspondientes del objetivo $y_n$, de forma que minimizaremos:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b859b",
   "metadata": {},
   "source": [
    "De forma que podemos resolver el problema de ajuste de curvas mediante la elecci√≥n de $w$ para la cual $E(w)$ sea lo m√°s peque√±a posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6e0b1",
   "metadata": {},
   "source": [
    "**Nota.** Dado que $E(w)$ es una funci√≥n cuadr√°tica de los coeficientes $w$, sus derivadas respecto a los coeficientes ser√°n lineales respecto a $w$, y el problema de minimizaci√≥n tendr√° soluci√≥n √∫nica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debf2db",
   "metadata": {},
   "source": [
    "### Expresi√≥n matricial de la funci√≥n objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eed8e",
   "metadata": {},
   "source": [
    "Antes de continuar, conviene obtener una representaci√≥n m√°s compacta del problema anterior. Comencemos por trabajar con el polinomio, d√°ndonos cuenta de que este es un producto punto entre los coeficientes $w$ y las potencias de $x$:\n",
    "\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* $x$ es la variable explicativa (escalar),\n",
    "* $w_0, w_1, \\dots, w_M$ son los par√°metros que queremos estimar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a3a3b",
   "metadata": {},
   "source": [
    "I. Vector de par√°metros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7e30b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#164ec6}{\\mathbf{w}} = \\underbrace{\\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M\\end{array}\\right]}_{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0fa64",
   "metadata": {},
   "source": [
    "II. Vector de caracter√≠sticas (o variables independientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32c806",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#00b050}{\\phi(x)^\\top} = \n",
    "\\underbrace{[1 \\quad x \\quad x^2 \\quad \\dots \\quad x^M]}_{\\phi(x)^\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f4995",
   "metadata": {},
   "source": [
    "III. Producto escalar o producto punto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae8a62",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x; w) \\;=\\; \\textcolor{#00b050}{\\phi(x)^\\top} \\textcolor{#164ec6}{\\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275adaaf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\color{purple}{\\Phi} =\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{#00b050}{\\phi(x_1)^\\top} \\\\\n",
    "\\textcolor{#00b050}{\\phi(x_2)^\\top} \\\\\n",
    "\\vdots \\\\\n",
    "\\textcolor{#00b050}{\\phi(x_N)^\\top}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041a945",
   "metadata": {},
   "source": [
    "IV. Matriz de dise√±o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1bda6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#800080}{\\Phi}\\,\\textcolor{#164ec6}{\\mathbf{w}} \\;=\\;\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\dots & x_1^M \\\\\n",
    "1 & x_2 & x_2^2 & \\dots & x_2^M \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_N & x_N^2 & \\dots & x_N^M\n",
    "\\end{bmatrix}\n",
    "\\left[\\begin{array}{c} \n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "w_M\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18648eb1",
   "metadata": {},
   "source": [
    "V. Predicciones para todos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd063601",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{y}} \\;=\\; \\Phi \\, \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3f44a",
   "metadata": {},
   "source": [
    "y, luego, ¬øqu√© necesitamos hacer a continuaci√≥n?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574e7b1",
   "metadata": {},
   "source": [
    "#### Norma euclidiana\n",
    "\n",
    "##### 1. La norma euclidiana: la idea general \n",
    "\n",
    "Primero recordemos la definici√≥n de la norma euclidiana, que simplemente mide la *\"longitud\"* de un vector en el espacio.\n",
    "\n",
    "Si tenemos un vector:\n",
    "\n",
    "$$\n",
    "v = \\left[\\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_s\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "la norma euclidiana de $v$ es:\n",
    "\n",
    "$$\n",
    "||v|| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_s^2},\n",
    "$$\n",
    "\n",
    "o equivalentemente $||v||^2 = v_1^2 + v_2^2 + \\dots + v_s^2 = \\sum_{i=1}^{s} v_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa54515",
   "metadata": {},
   "source": [
    "Con lo anterior, notemos que la funci√≥n de error la podemos reescribir en t√©rminos de la norma de un vector:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2 = \\frac{1}{2}\\sum_{n=1}^{N}\\left(\\phi(x_n)^T w - y_n\\right)^2 = \\frac{1}{2}||Z||^2,\n",
    "$$\n",
    "\n",
    "donde el vector $Z$ es:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\nonumber\n",
    "Z & = & \\left[\\begin{array}{c} \\phi(x_1)^T w - y_1 \\\\ \\phi(x_2)^T w - y_2 \\\\ \\vdots \\\\ \\phi(x_N)^T w - y_N\\end{array}\\right] \\\\ \\nonumber\n",
    "       & = & \\underbrace{\\left[\\begin{array}{ccc} - & \\phi(x_1)^T & - \\\\ - & \\phi(x_2)^T & - \\\\ & \\vdots & \\\\ - & \\phi(x_N)^T & -\\end{array}\\right]}_{\\Phi} w - \\underbrace{\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{array}\\right]}_{y} \\\\ \\nonumber\n",
    "       & = & \\Phi w - y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93b3cd",
   "metadata": {},
   "source": [
    "De este modo, queremos encontrar\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\arg \\min_{w} \\frac{1}{2} ||\\Phi w - y||^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dea000",
   "metadata": {},
   "source": [
    "### Ejercicio num√©rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolinomialFeatures y Phi (matriz de dise√±o)\n",
    "Phi = PolynomialFeatures(degree=2).fit_transform(x[:, None])\n",
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un modelo usando Pipeline\n",
    "model_3 = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qu√© contiene x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera una partici√≥n entre train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:, None], y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo\n",
    "model_3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los coeficientes de la regresi√≥n lineal\n",
    "model_3.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f80bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de entrenamiento?\n",
    "model_3.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47478c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de test?\n",
    "model_3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, as√≠ como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_3.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_3.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un nuevo modelo con grado 10\n",
    "model_10 = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=10)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo anterior\n",
    "model_10.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de entrenamiento?\n",
    "model_10.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de test?\n",
    "model_10.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, as√≠ como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1e6bf",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5eb4ef",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos definido la funci√≥n de error como\n",
    "\n",
    "$$\n",
    "\\textcolor{#960096}{E(\\mathbf{w}) \\;=\\; \\tfrac{1}{2}\\|\\Phi \\mathbf{w} - \\mathbf{y}\\|^2},\n",
    "$$\n",
    "\n",
    "donde la norma euclidiana mide el tama√±o del vector de errores \n",
    "$\\mathbf{e} = \\Phi \\mathbf{w} - \\mathbf{y}$.\n",
    "\n",
    "üëâ Sin embargo, **a√∫n no hemos puesto ninguna restricci√≥n sobre los par√°metros** \n",
    "$\\mathbf{w}$.  \n",
    "Si los pesos crecen demasiado (por ejemplo, al intentar ajustar exactamente todos los puntos de entrenamiento), podemos caer en **overfitting**: el modelo memoriza el ruido en lugar de generalizar.\n",
    "\n",
    "Para evitarlo, a√±adimos un nuevo t√©rmino a la funci√≥n objetivo que ahora tambi√©n \n",
    "mida la magnitud de los par√°metros. As√≠ obtenemos la **regresi√≥n Ridge**:\n",
    "\n",
    "$$\n",
    "E_{\\text{ridge}}(\\mathbf{w}) \n",
    "= \\textcolor{#960096}{\\tfrac{1}{2}\\|\\Phi \\mathbf{w} - \\mathbf{y}\\|^2}\n",
    "+ \\textcolor{#164ec6}{\\tfrac{\\lambda}{2}\\|\\mathbf{w}\\|^2},\n",
    "$$\n",
    "\n",
    "donde el hiperpar√°metro $\\lambda \\geq 0$ controla el equilibrio entre:\n",
    "\n",
    "- **Buen ajuste a los datos** (primer t√©rmino).  \n",
    "- **Mantener los par√°metros peque√±os** (segundo t√©rmino).  \n",
    "\n",
    "- Si $\\lambda = 0$, recuperamos la regresi√≥n normal.  \n",
    "- Si $\\lambda$ es grande, los pesos se reducen mucho y el modelo se vuelve m√°s simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694ac03",
   "metadata": {},
   "source": [
    "¬øqu√© soluci√≥n √≥ptima nos ofrece la regresi√≥n Ridge?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f751cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un pipeline, incluyendo ahora Ridge\n",
    "model_10_ridge = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=10)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo\n",
    "model_10_ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15157f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de entrenamiento?\n",
    "model_10_ridge.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øcu√°l es el score sobre los datos de test?\n",
    "model_10_ridge.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observa los coeficientes\n",
    "model_10_ridge.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9958be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, as√≠ como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10_ridge.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10_ridge.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2f285",
   "metadata": {},
   "source": [
    "*¬øqu√© pasa si incrementamos la cantidad de datos de entrenamiento?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera N=201\n",
    "\n",
    "N = 201\n",
    "x = np.linspace(0, 1, N)\n",
    "y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica la nube de puntos\n",
    "plt.plot(x, y, 'o', label='Datos', color='purple')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa en train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:, None], y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "model_10_ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56927441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara los scores de train y test\n",
    "train_score = model_10_ridge.score(x_train, y_train)\n",
    "test_score = model_10_ridge.score(x_test, y_test)\n",
    "\n",
    "print(f\"Train score: {train_score:.3f}, Test score: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, as√≠ como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10_ridge.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10_ridge.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1da90f",
   "metadata": {},
   "source": [
    "```{admonition} Sobre overfitting\n",
    ":class: tip\n",
    "\n",
    "![](../images/sesion5_regularizacion1.png)\n",
    "\n",
    "**Figura 1**: Lo que muestra es c√≥mo se ven las ‚Äúbolas de restricci√≥n‚Äù (constraint balls) en 3D para distintos tipos de regularizaci√≥n: Ridge ‚Äúencoge‚Äù los coeficientes pero dif√≠cilmente los lleva exactamente a cero; En Lasso, algunos coeficientes se vuelven exactamente cero; Elastic Net combina ambas propiedades. Hastie, *et al.*, 2020; Disponible en: https://arxiv.org/html/2006.00371v2. \n",
    "\n",
    "![](../images/sesion5_overfitting1.png)\n",
    "\n",
    "- **Punto rojo (OLS)**: m√≠nimo sin regularizaci√≥n (en el centro de los contornos).  \n",
    "\n",
    "- **Punto verde (√≥ptimo)**: soluci√≥n con regularizaci√≥n:  \n",
    "\n",
    "  - **Ridge (L2)**:  \n",
    "    $w^* \\;=\\; \\frac{\\mu}{1 + \\lambda_2}$ ‚Üí los coeficientes se *encogen* hacia el origen (**shrinkage**).  \n",
    "\n",
    "  - **Lasso (L1)**:  \n",
    "    $w^* \\;=\\; \\text{soft}(\\mu, \\tfrac{\\lambda_1}{2})$ ‚Üí algunos coeficientes se reducen a 0 (*sparsity*).  \n",
    "\n",
    "  - **Elastic Net (L1+L2)**:  \n",
    "    $w^* \\;=\\; \\frac{1}{1 + \\lambda_2}\\,\\text{soft}(\\mu, \\tfrac{\\lambda_1}{2})$ ‚Üí combinaci√≥n: *shrinkage* (L2) + *sparsity* (L1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c6f46",
   "metadata": {},
   "source": [
    "Para m√°s informaci√≥n acerca de regularizaci√≥n y overfitting:\n",
    "\n",
    "Hastie, T. (2020). *Ridge Regularization: an Essential Concept in Data Science.* Stanford University. [arXiv](https://arxiv.org/html/2006.00371v2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf7cfe",
   "metadata": {},
   "source": [
    "## Perspectiva probabil√≠stica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48954c9d",
   "metadata": {},
   "source": [
    "En OLS definimos el error de ajuste de manera puramente geom√©trica, como la suma de cuadrados de los residuos.  \n",
    "\n",
    "En cambio, en el enfoque probabil√≠stico de **M√°xima Verosimilitud (MLE)**, partimos de un modelo:\n",
    "\n",
    "$$\n",
    "y = \\phi(x)^{\\top} w + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Esto implica que, dado $x$, la variable de salida $y$ sigue una distribuci√≥n normal:\n",
    "\n",
    "$$\n",
    "p(y \\mid x, w) = \\mathcal{N}\\big(y \\mid \\phi(x)^{\\top} w, \\, \\beta^{-1}\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4f319",
   "metadata": {},
   "source": [
    "![](../images/sesion5-modeldistr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10f46d",
   "metadata": {},
   "source": [
    "> **Figura 1:** Los puntos negros representan las observaciones $(x_i, y_i)$.  \n",
    "> La l√≠nea azul es el valor esperado del modelo $\\phi(x)^\\top w$.  \n",
    "> Las curvas rojas muestran la distribuci√≥n emp√≠rica de los residuos, mientras que las curvas turquesa representan la distribuci√≥n normal te√≥rica asumida $\\mathcal{N}(0, \\beta^{-1})$.  \n",
    "> La figura ilustra que $p(y \\mid x,w) \\sim \\mathcal{N}(\\phi(x)^\\top w, \\beta^{-1})$, es decir, los datos se distribuyen alrededor de la recta con ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7578e7",
   "metadata": {},
   "source": [
    "Asumiendo independencia entre las observaciones, la **funci√≥n de verosimilitud** es:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\prod_{i=1}^N \\mathcal{N}(y_i \\mid \\phi(x_i)^{\\top} w, \\beta^{-1})\n",
    "$$\n",
    "\n",
    "y su logaritmo:\n",
    "\n",
    "\\begin{align}\n",
    "     & = \\sum_{i=1}^{N} \\log\\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1}) \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\sum_{i=1}^{N} (y_i - \\phi(x_i)^T w)^2 \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\left|\\left|y - \\Phi w\\right|\\right|^2\n",
    "\\end{align}\n",
    "\n",
    "Finalmente, maximizar $\\ell(w)$ respecto a $w$ es equivalente a minimizar:\n",
    "\n",
    "$$\n",
    "\\hat{w}_{MLE} = \\arg \\max_{w} l(w) = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2\n",
    "$$\n",
    "\n",
    "que es exactamente el mismo criterio de **OLS**.  \n",
    "\n",
    "As√≠, OLS surge como un **caso particular de MLE** bajo el supuesto de ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c6caa",
   "metadata": {},
   "source": [
    "![](../images/sesion5_mle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a137e",
   "metadata": {},
   "source": [
    "![](../images/sesion5_mle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019dcd3",
   "metadata": {},
   "source": [
    "Observamos que la estimaci√≥n de par√°metros por m√°xima verosimilitud, explica nuestra intuici√≥n detr√°s de m√≠nimos cuadrados.\n",
    "\n",
    "Adem√°s, **una vez m√°s concluimos que el enfoque de m√°xima verosimilitud nos puede traer problemas de overfitting**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
